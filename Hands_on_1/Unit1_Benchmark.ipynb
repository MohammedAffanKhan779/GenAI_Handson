{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "TrGAoyH2qnJ3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    \"BERT\": \"bert-base-uncased\",\n",
        "    \"RoBERTa\": \"roberta-base\",\n",
        "    \"BART\": \"facebook/bart-base\"\n",
        "}"
      ],
      "metadata": {
        "id": "vX7gzoIVqjTz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_generation(models):\n",
        "    print(\"\\n===== TEXT GENERATION BENCHMARK =====\")\n",
        "    prompt = \"The future of Artificial Intelligence is\"\n",
        "\n",
        "    for name, model_id in models.items():\n",
        "        print(f\"\\n{name}:\")\n",
        "        try:\n",
        "            gen = pipeline(\"text-generation\", model=model_id)\n",
        "            out = gen(prompt, max_length=30)\n",
        "            print(\"Status: SUCCESS\")\n",
        "            print(\"Output:\", out[0][\"generated_text\"][:100])\n",
        "        except Exception as e:\n",
        "            print(\"Status: FAILURE\")\n",
        "            print(\"Reason:\", e)\n",
        "\n"
      ],
      "metadata": {
        "id": "v96YsaApq3Vm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_generation(models)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpQUGcN0twxV",
        "outputId": "55214542-b610-4f7a-ae05-1d1fe5fd4526"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== TEXT GENERATION BENCHMARK =====\n",
            "\n",
            "BERT:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Status: SUCCESS\n",
            "Output: The future of Artificial Intelligence is............................................................\n",
            "\n",
            "RoBERTa:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Status: SUCCESS\n",
            "Output: The future of Artificial Intelligence is\n",
            "\n",
            "BART:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Status: SUCCESS\n",
            "Output: The future of Artificial Intelligence isachaacha Idaho Johns droid droid raided outweigh correctraph\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_fill_mask(models):\n",
        "    print(\"\\n===== FILL-MASK BENCHMARK =====\")\n",
        "\n",
        "    sentences = {\n",
        "        \"BERT\": \"The goal of Generative AI is to [MASK] new content.\",\n",
        "        \"RoBERTa\": \"The goal of Generative AI is to <mask> new content.\",\n",
        "        \"BART\": \"The goal of Generative AI is to <mask> new content.\"\n",
        "    }\n",
        "\n",
        "    for name, model_id in models.items():\n",
        "        print(f\"\\n{name}:\")\n",
        "        try:\n",
        "            fill = pipeline(\"fill-mask\", model=model_id)\n",
        "            outputs = fill(sentences[name])\n",
        "            print(\"Status: SUCCESS\")\n",
        "            print(\"Top predictions:\")\n",
        "            for o in outputs[:3]:\n",
        "                print(\"-\", o[\"token_str\"])\n",
        "        except Exception as e:\n",
        "            print(\"Status: FAILURE\")\n",
        "            print(\"Reason:\", e)"
      ],
      "metadata": {
        "id": "Pz8BFLr1sZCx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_fill_mask(models)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dW4CUUmwuL0p",
        "outputId": "18b124fa-480e-4c60-8ea3-3e42eae3909c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== FILL-MASK BENCHMARK =====\n",
            "\n",
            "BERT:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Status: SUCCESS\n",
            "Top predictions:\n",
            "- create\n",
            "- generate\n",
            "- produce\n",
            "\n",
            "RoBERTa:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Status: SUCCESS\n",
            "Top predictions:\n",
            "-  generate\n",
            "-  create\n",
            "-  discover\n",
            "\n",
            "BART:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Status: SUCCESS\n",
            "Top predictions:\n",
            "-  create\n",
            "-  help\n",
            "-  provide\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_qa(models):\n",
        "    print(\"\\n===== QUESTION ANSWERING BENCHMARK =====\")\n",
        "\n",
        "    context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        "    question = \"What are the risks?\"\n",
        "\n",
        "    for name, model_id in models.items():\n",
        "        print(f\"\\n{name}:\")\n",
        "        try:\n",
        "            qa = pipeline(\"question-answering\", model=model_id)\n",
        "            answer = qa(question=question, context=context)\n",
        "            print(\"Status: SUCCESS\")\n",
        "            print(\"Answer:\", answer[\"answer\"])\n",
        "            print(\"Score:\", round(answer[\"score\"], 4))\n",
        "        except Exception as e:\n",
        "            print(\"Status: FAILURE\")\n",
        "            print(\"Reason:\", e)\n"
      ],
      "metadata": {
        "id": "HXk6kfUGuUAE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_qa(models)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoLgZylyuXX8",
        "outputId": "ec1a47cc-da53-469e-bc9d-f44ae24dbb09"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== QUESTION ANSWERING BENCHMARK =====\n",
            "\n",
            "BERT:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Status: SUCCESS\n",
            "Answer: , and deepfakes\n",
            "Score: 0.0095\n",
            "\n",
            "RoBERTa:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Status: SUCCESS\n",
            "Answer: Generative\n",
            "Score: 0.0073\n",
            "\n",
            "BART:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Status: SUCCESS\n",
            "Answer: such\n",
            "Score: 0.0333\n"
          ]
        }
      ]
    }
  ]
}